{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3072)\n",
      "(2000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "X = np.genfromtxt ('X32.csv', delimiter=\",\")\n",
    "y = np.genfromtxt ('y32.csv', delimiter=\",\")\n",
    "\n",
    "print X.shape\n",
    "\n",
    "X = X.reshape(2000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "#X = a.reshape(349,3,32,32).swapaxes(0,2)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1600, 32, 32, 3)\n",
      "Training labels shape:  (1600,)\n",
      "Test data shape:  (400, 32, 32, 3)\n",
      "Test labels shape:  (400,)\n",
      "Train data shape:  (3073, 1200)\n",
      "Train labels shape:  (1200,)\n",
      "Validation data shape:  (3073, 400)\n",
      "Validation labels shape:  (400,)\n",
      "Test data shape:  (3073, 400)\n",
      "Test labels shape:  (400,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=1200, num_validation=400, num_test=400):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  #X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  \n",
    "  X_train = X[0:1600]\n",
    "  y_train = y[0:1600]\n",
    "  X_test = X[1600:]\n",
    "  y_test = y[1600:]\n",
    "\n",
    "  print 'Training data shape: ', X_train.shape\n",
    "  print 'Training labels shape: ', y_train.shape\n",
    "  print 'Test data shape: ', X_test.shape\n",
    "  print 'Test labels shape: ', y_test.shape\n",
    "\n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]).T\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]).T\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]).T\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "#y_train[:400] = 0\n",
    "#y_train[800:1200] = 1\n",
    "y_test[0:100] = 1;\n",
    "y_test[300:] = 0;\n",
    "y_val[0:50] = 1;\n",
    "y_val[350:] = 0;\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/classifiers/softmax.py:45: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  loss += -f_i[y[i]] + np.log(sum_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.438260\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(10, 3073) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_train, y_train, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.280803 analytic: 0.280802, relative error: 6.158515e-08\n",
      "numerical: -0.986748 analytic: -0.986748, relative error: 1.161441e-07\n",
      "numerical: 0.828909 analytic: 0.828909, relative error: 9.443423e-08\n",
      "numerical: 26.799676 analytic: 26.799676, relative error: 3.010466e-09\n",
      "numerical: -2.264449 analytic: -2.264448, relative error: 5.557613e-08\n",
      "numerical: -17.485060 analytic: -17.485060, relative error: 6.162827e-09\n",
      "numerical: 28.493667 analytic: 28.493668, relative error: 9.606591e-09\n",
      "numerical: -1.147677 analytic: -1.147677, relative error: 4.586164e-08\n",
      "numerical: 13.332838 analytic: 13.332838, relative error: 1.820432e-09\n",
      "numerical: 0.313850 analytic: 0.313850, relative error: 1.827293e-07\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_train, y_train, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_train, y_train, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ..., 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.438260e+00 computed in 0.231636s\n",
      "vectorized loss: 9.534754e+00 computed in 0.018360s\n",
      "Loss difference: 7.096493\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_train, y_train, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_train, y_train.astype(int), 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/classifiers/softmax.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean( np.log(np.exp(f_correct)/np.sum(np.exp(f))) )\n",
      "cs231n/classifiers/softmax.py:98: RuntimeWarning: invalid value encountered in divide\n",
      "  p = np.exp(f)/np.sum(np.exp(f), axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.000000e-10 reg 1.000000e-03 train accuracy: 0.900833 val accuracy: 0.780000\n",
      "lr 1.000000e-10 reg 1.000000e-02 train accuracy: 0.177500 val accuracy: 0.265000\n",
      "lr 1.000000e-10 reg 1.000000e-01 train accuracy: 0.509167 val accuracy: 0.505000\n",
      "lr 1.000000e-10 reg 1.000000e+00 train accuracy: 0.229167 val accuracy: 0.347500\n",
      "lr 1.000000e-10 reg 1.000000e+01 train accuracy: 0.252500 val accuracy: 0.312500\n",
      "lr 1.000000e-10 reg 1.000000e+02 train accuracy: 0.734167 val accuracy: 0.642500\n",
      "lr 1.000000e-10 reg 1.000000e+03 train accuracy: 0.692500 val accuracy: 0.697500\n",
      "lr 1.000000e-10 reg 1.000000e+04 train accuracy: 0.094167 val accuracy: 0.200000\n",
      "lr 1.000000e-10 reg 1.000000e+05 train accuracy: 0.924167 val accuracy: 0.802500\n",
      "lr 1.000000e-10 reg 1.000000e+06 train accuracy: 0.440000 val accuracy: 0.435000\n",
      "lr 1.668101e-08 reg 1.000000e-03 train accuracy: 0.416667 val accuracy: 0.460000\n",
      "lr 1.668101e-08 reg 1.000000e-02 train accuracy: 0.182500 val accuracy: 0.235000\n",
      "lr 1.668101e-08 reg 1.000000e-01 train accuracy: 0.926667 val accuracy: 0.812500\n",
      "lr 1.668101e-08 reg 1.000000e+00 train accuracy: 0.661667 val accuracy: 0.575000\n",
      "lr 1.668101e-08 reg 1.000000e+01 train accuracy: 0.595000 val accuracy: 0.527500\n",
      "lr 1.668101e-08 reg 1.000000e+02 train accuracy: 0.838333 val accuracy: 0.722500\n",
      "lr 1.668101e-08 reg 1.000000e+03 train accuracy: 0.043333 val accuracy: 0.175000\n",
      "lr 1.668101e-08 reg 1.000000e+04 train accuracy: 0.538333 val accuracy: 0.502500\n",
      "lr 1.668101e-08 reg 1.000000e+05 train accuracy: 0.475833 val accuracy: 0.522500\n",
      "lr 1.668101e-08 reg 1.000000e+06 train accuracy: 0.205000 val accuracy: 0.282500\n",
      "lr 2.782559e-06 reg 1.000000e-03 train accuracy: 0.990000 val accuracy: 0.840000\n",
      "lr 2.782559e-06 reg 1.000000e-02 train accuracy: 1.000000 val accuracy: 0.857500\n",
      "lr 2.782559e-06 reg 1.000000e-01 train accuracy: 0.995833 val accuracy: 0.847500\n",
      "lr 2.782559e-06 reg 1.000000e+00 train accuracy: 0.994167 val accuracy: 0.847500\n",
      "lr 2.782559e-06 reg 1.000000e+01 train accuracy: 1.000000 val accuracy: 0.857500\n",
      "lr 2.782559e-06 reg 1.000000e+02 train accuracy: 0.996667 val accuracy: 0.852500\n",
      "lr 2.782559e-06 reg 1.000000e+03 train accuracy: 0.993333 val accuracy: 0.847500\n",
      "lr 2.782559e-06 reg 1.000000e+04 train accuracy: 0.993333 val accuracy: 0.852500\n",
      "lr 2.782559e-06 reg 1.000000e+05 train accuracy: 1.000000 val accuracy: 0.857500\n",
      "lr 2.782559e-06 reg 1.000000e+06 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 4.641589e-04 reg 1.000000e-03 train accuracy: 1.000000 val accuracy: 0.857500\n",
      "lr 4.641589e-04 reg 1.000000e-02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 4.641589e-04 reg 1.000000e-01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 4.641589e-04 reg 1.000000e+00 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 4.641589e-04 reg 1.000000e+01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 4.641589e-04 reg 1.000000e+02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 4.641589e-04 reg 1.000000e+03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 4.641589e-04 reg 1.000000e+04 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 4.641589e-04 reg 1.000000e+05 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 4.641589e-04 reg 1.000000e+06 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 7.742637e-02 reg 1.000000e-03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 7.742637e-02 reg 1.000000e-02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 7.742637e-02 reg 1.000000e-01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 7.742637e-02 reg 1.000000e+00 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 7.742637e-02 reg 1.000000e+01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 7.742637e-02 reg 1.000000e+02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 7.742637e-02 reg 1.000000e+03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 7.742637e-02 reg 1.000000e+04 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 7.742637e-02 reg 1.000000e+05 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 7.742637e-02 reg 1.000000e+06 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.291550e+01 reg 1.000000e-03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.291550e+01 reg 1.000000e-02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.291550e+01 reg 1.000000e-01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.291550e+01 reg 1.000000e+00 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.291550e+01 reg 1.000000e+01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.291550e+01 reg 1.000000e+02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.291550e+01 reg 1.000000e+03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.291550e+01 reg 1.000000e+04 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.291550e+01 reg 1.000000e+05 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.291550e+01 reg 1.000000e+06 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 2.154435e+03 reg 1.000000e-03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 2.154435e+03 reg 1.000000e-02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 2.154435e+03 reg 1.000000e-01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 2.154435e+03 reg 1.000000e+00 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 2.154435e+03 reg 1.000000e+01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 2.154435e+03 reg 1.000000e+02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 2.154435e+03 reg 1.000000e+03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 2.154435e+03 reg 1.000000e+04 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 2.154435e+03 reg 1.000000e+05 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 2.154435e+03 reg 1.000000e+06 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 3.593814e+05 reg 1.000000e-03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 3.593814e+05 reg 1.000000e-02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 3.593814e+05 reg 1.000000e-01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 3.593814e+05 reg 1.000000e+00 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 3.593814e+05 reg 1.000000e+01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 3.593814e+05 reg 1.000000e+02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 3.593814e+05 reg 1.000000e+03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 3.593814e+05 reg 1.000000e+04 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 3.593814e+05 reg 1.000000e+05 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 3.593814e+05 reg 1.000000e+06 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 5.994843e+07 reg 1.000000e-03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 5.994843e+07 reg 1.000000e-02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 5.994843e+07 reg 1.000000e-01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 5.994843e+07 reg 1.000000e+00 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 5.994843e+07 reg 1.000000e+01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 5.994843e+07 reg 1.000000e+02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 5.994843e+07 reg 1.000000e+03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 5.994843e+07 reg 1.000000e+04 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 5.994843e+07 reg 1.000000e+05 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 5.994843e+07 reg 1.000000e+06 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.000000e+10 reg 1.000000e-03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.000000e+10 reg 1.000000e-02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.000000e+10 reg 1.000000e-01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.000000e+10 reg 1.000000e+00 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.000000e+10 reg 1.000000e+01 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.000000e+10 reg 1.000000e+02 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.000000e+10 reg 1.000000e+03 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.000000e+10 reg 1.000000e+04 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.000000e+10 reg 1.000000e+05 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "lr 1.000000e+10 reg 1.000000e+06 train accuracy: 0.495000 val accuracy: 0.482500\n",
      "best validation accuracy achieved during cross-validation: 0.857500\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.logspace(-10, 10, 10) # np.logspace(-10, 10, 8) #-10, -9, -8, -7, -6, -5, -4\n",
    "regularization_strengths = np.logspace(-3, 6, 10) # causes numeric issues: np.logspace(-5, 5, 8) #[-4, -3, -2, -1, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "################################################################################\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "iters = 10 #100\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train.astype(int), learning_rate=lr, reg=rs, num_iters=iters)\n",
    "        \n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        acc_train = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        acc_val = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        results[(lr, rs)] = (acc_train, acc_val)\n",
    "        \n",
    "        if best_val < acc_val:\n",
    "            best_val = acc_val\n",
    "            best_softmax = softmax\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.765000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best svm on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOoAAAHpCAYAAACMd6BUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQjiAjCqEA8ioQjiAjCqEA/xf6YXjYpeQWLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f389e854b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:,:-1] # strip out the bias\n",
    "w = w.reshape(2, 32, 32, 3)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['no', 'yes']\n",
    "for i in xrange(2):\n",
    "  plt.subplot(2, 1, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165,  43],\n",
       "       [ 51, 141]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
